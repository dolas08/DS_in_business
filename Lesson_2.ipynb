{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7464c24c",
   "metadata": {},
   "source": [
    "1) Самостоятельно разобраться с тем, что такое tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html и еще - https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "2) Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог с помощью precision_recall_curve, как это делалось на уроке)\n",
    "\n",
    "3) Повторить п.2, но используя уже не медиану, а max\n",
    "\n",
    "4) (опциональное) Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (подсказка: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал). Подсказка 2 - нужен именно idf, как вес.\n",
    "\n",
    "5) Сформировать на выходе единую таблицу, сравнивающую качество 4 разных методов получения эмбедингов пользователей: mean, median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
    "\n",
    "6) Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cebdaf2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import itertools\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from razdel import tokenize\n",
    "import pymorphy2\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2947b28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Dolas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b96fec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                        articles\n",
       "0  u105138  [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1  u108690          [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2  u108339          [1845, 2009, 2356, 1424, 2939, 323389]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users = pd.read_csv(\"users_articles.csv\")\n",
    "users.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c352522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Заместитель председателяnправительства РФnСерг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>Матч 1/16 финала Кубка России по футболу был п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>Форвард «Авангарда» Томаш Заборский прокоммент...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title\n",
       "0       6  Заместитель председателяnправительства РФnСерг...\n",
       "1    4896  Матч 1/16 финала Кубка России по футболу был п...\n",
       "2    4897  Форвард «Авангарда» Томаш Заборский прокоммент..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news = pd.read_csv(\"materials.csv\")\n",
    "print(news.shape)\n",
    "news.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0038c575",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_ru = stopwords.words('russian')\n",
    "len(stopword_ru)\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "76f954ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopword_ru += additional_stopwords\n",
    "len(stopword_ru)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "388772c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2231c49c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-7ee348d9b386>:15: FutureWarning: Possible nested set at position 39\n",
      "  text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 21.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста. Будет долго...\n",
    "news['title'] = news['title'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20212da5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста. Будет очень долго...\n",
    "news['title'] = news['title'].apply(lambda x: lemmatization(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f461653d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in news['title'].values]\n",
    "\n",
    "# Create a corpus from a list of texts\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d60a8882",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92339419",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d705c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 22.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from gensim.models import LdaModel\n",
    "# Train the model on the corpus.\n",
    "lda = LdaModel(common_corpus, num_topics=25, id2word=common_dictionary)#, passes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "682f8273",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# Save model to disk.\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)\n",
    "\n",
    "# Load a potentially pretrained model from disk.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2472fb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['форвард', 'авангард', 'томаш', 'заборский', 'прокомментировать', 'игра', 'свой', 'команда', 'матч', 'чемпионат', 'кхл', 'против', 'атланта', 'nnnn', 'провести', 'плохой', 'матч', 'нижний', 'новгород', 'против', 'торпедо', 'настраиваться', 'первый', 'минута', 'включиться', 'работа', 'сказать', 'заборский', 'получиться', 'забросить', 'быстрый', 'гол', 'задать', 'хороший', 'темп', 'поединок', 'мочь', 'играть', 'ещё', 'хороший', 'сторона', 'пять', 'очко', 'выезд', 'девять', 'это', 'хороший']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 0.028580407),\n",
       " (7, 0.27453008),\n",
       " (9, 0.43868056),\n",
       " (14, 0.16412441),\n",
       " (19, 0.050772034),\n",
       " (22, 0.026365167)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new corpus, made of previously unseen documents.\n",
    "other_texts = [t for t in news['title'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e4b716ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: год который это исследование первый время весь\n",
      "topic_1: тело смерть женщина врач трагедия выдать реакция\n",
      "topic_2: экипаж гостиница порт рим молдавия эндрю перо\n",
      "topic_3: украина украинский киев сигнал вуз это человек\n",
      "topic_4: дело сотрудник суд человек полиция уголовный орган\n",
      "topic_5: женщина мужчина египет летний высота бизнесмен тело\n",
      "topic_6: потенциально шутка присудить скотт фантазия стерлинг аннулировать\n",
      "topic_7: это который мочь год человек свой всё\n",
      "topic_8: год это который nn млн рубль проект\n",
      "topic_9: достигать германия польша игра сон италия тепло\n",
      "topic_10: ракета км километр перевод пилотировать горизонт секунда\n",
      "topic_11: остров япония карта сектор мэй берег выдавать\n",
      "topic_12: дождь офицер водитель скорость инвестировать корь многолетний\n",
      "topic_13: рак место фильм nn кость год фрагмент\n",
      "topic_14: конструкция лауреат рт добраться кг балл столетие\n",
      "topic_15: обнаружить это президент путин данные источник государство\n",
      "topic_16: северный земля запуск умереть следствие океан употребление\n",
      "topic_17: это который глава область рейс год nn\n",
      "topic_18: год россия также погибнуть рф который санкция\n",
      "topic_19: газ взрыв энергия вода великобритания медицина британский\n",
      "topic_20: год млрд россия фонд программа тыс температура\n",
      "topic_21: больной гражданство аналог кухня альтернатива дорого совпасть\n",
      "topic_22: журнал ск подросток подсчитать индия праздничный свердловский\n",
      "topic_23: статья закон год тыс который законопроект поверхность\n",
      "topic_24: год цена который снижение дом тыс это\n"
     ]
    }
   ],
   "source": [
    "x=lda.show_topics(num_topics=25, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "\n",
    "#Below Code Prints Only Words \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "606dfc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = news['title'].iloc[0]\n",
    "\n",
    "def get_lda_vector(text):\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5100ac2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014791</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.849105</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.542823</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.02858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.274536</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050772</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.026365</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.671976</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.072674</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.032717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.194802</td>\n",
       "      <td>0.649618</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id   topic_0  topic_1  topic_2  topic_3  topic_4  topic_5  topic_6  \\\n",
       "0       6  0.000000      0.0      0.0      0.0      0.0  0.00000      0.0   \n",
       "1    4896  0.000000      0.0      0.0      0.0      0.0  0.00000      0.0   \n",
       "2    4897  0.000000      0.0      0.0      0.0      0.0  0.02858      0.0   \n",
       "3    4898  0.671976      0.0      0.0      0.0      0.0  0.00000      0.0   \n",
       "4    4899  0.000000      0.0      0.0      0.0      0.0  0.00000      0.0   \n",
       "\n",
       "    topic_7   topic_8  ...  topic_15  topic_16  topic_17  topic_18  topic_19  \\\n",
       "0  0.000000  0.014791  ...       0.0       0.0  0.000000  0.849105  0.000000   \n",
       "1  0.000000  0.000000  ...       0.0       0.0  0.000000  0.000000  0.542823   \n",
       "2  0.274536  0.000000  ...       0.0       0.0  0.000000  0.000000  0.050772   \n",
       "3  0.000000  0.000000  ...       0.0       0.0  0.000000  0.072674  0.000000   \n",
       "4  0.000000  0.000000  ...       0.0       0.0  0.194802  0.649618  0.000000   \n",
       "\n",
       "   topic_20  topic_21  topic_22  topic_23  topic_24  \n",
       "0       0.0       0.0  0.000000       0.0  0.000000  \n",
       "1       0.0       0.0  0.000000       0.0  0.000000  \n",
       "2       0.0       0.0  0.026365       0.0  0.000000  \n",
       "3       0.0       0.0  0.000000       0.0  0.032717  \n",
       "4       0.0       0.0  0.000000       0.0  0.000000  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(25)]]\n",
    "topic_matrix.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "82f910cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(25)]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d87465b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_articles_list = users['articles'].iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b2e43e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(user_articles_list, mode):\n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    if mode == 'mean':\n",
    "        user_vector = np.mean(user_vector, 0)\n",
    "    if mode == 'median':\n",
    "        user_vector = np.median(user_vector, 0)\n",
    "    if mode == 'max':\n",
    "        user_vector = np.amax(user_vector, 0)\n",
    "    if mode == 'idf_mean':\n",
    "        return np.nan\n",
    "    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be0208fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_user_embedding(user_articles_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "651c4a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
    "# user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "# user_embeddings['uid'] = users['uid'].values\n",
    "# user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "# user_embeddings.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7bfd6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# target = pd.read_csv(\"users_churn.csv\")\n",
    "# target.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8064d091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = pd.merge(user_embeddings, target, 'left')\n",
    "# X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc9f62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e58df42a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "#import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "85869035",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "72199ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #разделим данные на train/test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]], \n",
    "#                                                     X['churn'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bf76f870",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logreg = LogisticRegression()\n",
    "# #обучим \n",
    "# logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7e0dac69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #наши прогнозы для тестовой выборки\n",
    "# preds = logreg.predict_proba(X_test)[:, 1]\n",
    "# preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a55caf93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5686bb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a6e59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d53fb030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "# fscore = (2 * precision * recall) / (precision + recall)\n",
    "# # locate the index of the largest f score\n",
    "# ix = np.argmax(fscore)\n",
    "# print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "#                                                                         fscore[ix],\n",
    "#                                                                         precision[ix],\n",
    "#                                                                         recall[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "8f14ec7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87e19425",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #мы уже нашли ранее \"оптимальный\" порог, когда максимизировали f_score\n",
    "# font = {'size' : 15}\n",
    "\n",
    "# plt.rc('font', **font)\n",
    "\n",
    "# cnf_matrix = confusion_matrix(y_test, preds>thresholds[ix])\n",
    "# plt.figure(figsize=(10, 8))\n",
    "# plot_confusion_matrix(cnf_matrix, classes=['Non-Churn', 'churn'],\n",
    "#                       title='Confusion matrix')\n",
    "# plt.savefig(\"conf_matrix.png\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f9b8a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0e96c78",
   "metadata": {},
   "source": [
    "         roc_auc| precision| recall| f_score\n",
    "         ___________________________________\n",
    "mean    |       |          |       |        |\n",
    "median  |       |          |       |        |\n",
    "max     |       |          |       |        |\n",
    "idf_mean|       |          |       |        |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "3194dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def task_2(mode, user_articles_list, doc_dict, users):\n",
    "    output_data = []\n",
    "    if mode == 'mean':\n",
    "        \n",
    "        user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, mode), 1)])\n",
    "        user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "        user_embeddings['uid'] = users['uid'].values\n",
    "        user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "        \n",
    "        target = pd.read_csv(\"users_churn.csv\")\n",
    "        X = pd.merge(user_embeddings, target, 'left')\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]], \n",
    "                                                    X['churn'], random_state=0)\n",
    "        logreg = LogisticRegression()\n",
    "        logreg.fit(X_train, y_train)\n",
    "        preds = logreg.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        # locate the index of the largest f score\n",
    "        ix = np.argmax(fscore)\n",
    "        print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f, roc_auc_score=%.6f'  % (thresholds[ix], \n",
    "                                                                              fscore[ix],\n",
    "                                                                                precision[ix],\n",
    "                                                                                recall[ix], roc_auc_score(y_test, preds)))\n",
    "        output_data = [fscore[ix],precision[ix],recall[ix], roc_auc_score(y_test, preds)]\n",
    "        return output_data\n",
    "    \n",
    "    if mode == 'median':\n",
    "        \n",
    "        \n",
    "        user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, mode), 1)])\n",
    "        user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "        user_embeddings['uid'] = users['uid'].values\n",
    "        user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "        \n",
    "        target = pd.read_csv(\"users_churn.csv\")\n",
    "        X = pd.merge(user_embeddings, target, 'left')\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]], \n",
    "                                                    X['churn'], random_state=0)\n",
    "        logreg = LogisticRegression()\n",
    "        logreg.fit(X_train, y_train)\n",
    "        preds = logreg.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        # locate the index of the largest f score\n",
    "        ix = np.argmax(fscore)\n",
    "        print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f, roc_auc_score=%.6f'  % (thresholds[ix], \n",
    "                                                                                fscore[ix],\n",
    "                                                                                precision[ix],\n",
    "                                                                                recall[ix], roc_auc_score(y_test, preds)))\n",
    "        output_data = [fscore[ix],precision[ix],recall[ix], roc_auc_score(y_test, preds)]\n",
    "        return output_data\n",
    "    \n",
    "    if mode == 'max':\n",
    "        \n",
    "        \n",
    "        user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, mode), 1)])\n",
    "        user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "        user_embeddings['uid'] = users['uid'].values\n",
    "        user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "        \n",
    "        target = pd.read_csv(\"users_churn.csv\")\n",
    "        X = pd.merge(user_embeddings, target, 'left')\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]], \n",
    "                                                    X['churn'], random_state=0)\n",
    "        logreg = LogisticRegression()\n",
    "        logreg.fit(X_train, y_train)\n",
    "        preds = logreg.predict_proba(X_test)[:, 1]\n",
    "        precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "        fscore = (2 * precision * recall) / (precision + recall)\n",
    "        # locate the index of the largest f score\n",
    "        ix = np.argmax(fscore)\n",
    "        print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f, roc_auc_score=%.6f'  % (thresholds[ix], \n",
    "                                                                                fscore[ix],\n",
    "                                                                                precision[ix],\n",
    "                                                                                recall[ix], roc_auc_score(y_test, preds)))\n",
    "        output_data = [fscore[ix],precision[ix],recall[ix], roc_auc_score(y_test, preds)]\n",
    "        return output_data\n",
    "    if mode == 'idf_mean':\n",
    "        pass\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "398b0962",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Threshold=0.295787, F-Score=0.693, Precision=0.700, Recall=0.686, roc_auc_score=0.946118\n",
      "Best Threshold=0.260213, F-Score=0.718, Precision=0.638, Recall=0.820, roc_auc_score=0.963033\n",
      "Best Threshold=0.358469, F-Score=0.809, Precision=0.806, Recall=0.812, roc_auc_score=0.978406\n"
     ]
    }
   ],
   "source": [
    "tests = ['mean', 'median', 'max', 'idf_mean']\n",
    "tests_result = []\n",
    "tests_result_label = ['F-Score', 'Precision', 'Recall', 'roc_auc_score']\n",
    "for each in tests:\n",
    "    tests_result.append(task_2(each, user_articles_list, doc_dict, users))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "40e6e46d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>F-Score</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>roc_auc_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.692784</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>0.685714</td>\n",
       "      <td>0.946118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>median</th>\n",
       "      <td>0.717857</td>\n",
       "      <td>0.638095</td>\n",
       "      <td>0.820408</td>\n",
       "      <td>0.963033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.808943</td>\n",
       "      <td>0.805668</td>\n",
       "      <td>0.812245</td>\n",
       "      <td>0.978406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>idf_mean</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           F-Score  Precision    Recall  roc_auc_score\n",
       "mean      0.692784   0.700000  0.685714       0.946118\n",
       "median    0.717857   0.638095  0.820408       0.963033\n",
       "max       0.808943   0.805668  0.812245       0.978406\n",
       "idf_mean       NaN        NaN       NaN            NaN"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(tests_result, columns=tests_result_label, index=tests)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8136b631",
   "metadata": {},
   "source": [
    "разница из-за того, что мы смотрим на вектор в трёх (четырёх) вариантах, один медиана, один средний и один максимальный\n",
    "где в среднем пользователь смотрел на эти темы\n",
    "или где располагается медианное значение среди тем\n",
    "и максимальное больше всех, потому что пользователь смотрел больше всего этих тем (максимальное среди тем)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf29651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
